"""
Telegram Service com TDLib - Performance M√°xima
Substitui TelegramServiceBalanced do Telethon

Performance esperada:
- 100-150k mensagens/dia (vs 20k com Telethon)
- 4000-6000 mensagens/hora
- Rate limits muito mais generosos
"""
import asyncio
import time
import logging
from typing import List, Dict, Optional
from app.services.now.tdlib_session_pool import TDLibSessionPool
from app.services.now.tdlib_config import TDLibConfig

# Seus reposit√≥rios existentes
from app.repository.groups_repository import GroupsRepository
from app.repository.credentials_repository import CredentialsRepository
from app.repository.group_credentials_repository import GroupCredentialsRepository
from app.repository.media_repository import MediaRepository
from config.settings import Config

logger = logging.getLogger(__name__)


class TDLibTelegramService:
    """
    Servi√ßo Telegram com TDLib - Alta Performance

    Diferen√ßas vs Telethon:
    - 5x mais r√°pido
    - Rate limits muito menores
    - Melhor gerenciamento de recursos
    - Usado pelos apps oficiais do Telegram
    """

    def __init__(self):
        # Reposit√≥rios (mant√©m os mesmos)
        self.groups_repo = GroupsRepository()
        self.creds_repo = CredentialsRepository()
        self.group_creds_repo = GroupCredentialsRepository()
        self.media_repo = MediaRepository()

        # Configura√ß√µes
        self.num_workers = Config.NUM_WORKERS
        self.msg_per_worker = Config.MSG_POR_WORKER

        # TDLib aguenta muito mais concorr√™ncia
        self.max_concurrent_downloads = TDLibConfig.MAX_CONCURRENT_DOWNLOADS

        logger.info(f"üöÄ TDLib Service inicializado")
        logger.info(f"‚öôÔ∏è  Workers: {self.num_workers} | Msgs/worker: {self.msg_per_worker}")
        logger.info(f"‚ö° Concorr√™ncia: {self.max_concurrent_downloads} downloads simult√¢neos")

    async def run_all_groups(self):
        """
        Processa todos os grupos habilitados

        Fluxo:
        1. Busca grupos habilitados
        2. Para cada grupo:
           - Conecta com credencial
           - Busca mensagens n√£o processadas
           - Faz download paralelo
           - Processa pipeline
        """
        groups = self.groups_repo.where("enabled", 1).get()

        if not groups:
            logger.warning("‚ö†Ô∏è Nenhum grupo habilitado no banco")
            return

        logger.info(f"\n{'='*70}")
        logger.info(f"üìã {len(groups)} grupo(s) habilitado(s) para processamento")
        logger.info(f"{'='*70}\n")

        total_start = time.time()

        for idx, group in enumerate(groups, 1):
            logger.info(f"\n{'='*70}")
            logger.info(f"üéØ [{idx}/{len(groups)}] Grupo: {group['title']}")
            logger.info(f"   ID: {group['id']} | Last Update: {group.get('last_update_id', 0)}")
            logger.info(f"{'='*70}")

            try:
                await self._process_group(group)

            except Exception as e:
                logger.error(f"‚ùå Erro fatal no grupo {group['title']}: {e}", exc_info=True)

            # Pausa suave entre grupos (TDLib n√£o precisa de muito tempo)
            if idx < len(groups):
                logger.info("‚è≥ Aguardando 2s antes do pr√≥ximo grupo...\n")
                await asyncio.sleep(2)

        total_elapsed = time.time() - total_start
        logger.info(f"\n{'='*70}")
        logger.info(f"‚úÖ TODOS OS GRUPOS PROCESSADOS")
        logger.info(f"‚è±Ô∏è  Tempo total: {total_elapsed:.0f}s ({total_elapsed/60:.1f} min)")
        logger.info(f"{'='*70}\n")

    async def _process_group(self, group: dict):
        """Processa um grupo espec√≠fico"""
        start_time = time.time()

        # 1Ô∏è‚É£ Busca credencial do grupo
        link = self.group_creds_repo.where("group_id", group["id"]).first()
        if not link:
            logger.warning("‚ö†Ô∏è Grupo sem credencial vinculada")
            return

        cred = self.creds_repo.find(link["credential_id"])
        if not cred or not cred.get("active"):
            logger.warning("‚ö†Ô∏è Credencial inv√°lida ou inativa")
            return

        logger.info(f"üîë Credencial: {cred.get('session_name', 'N/A')}")

        # 2Ô∏è‚É£ Inicializa pool TDLib
        session_pool = TDLibSessionPool(cred)

        try:
            # N√∫mero de sess√µes baseado na configura√ß√£o
            num_sessions = min(5, TDLibConfig.MAX_CONCURRENT_SESSIONS)

            if not await session_pool.initialize(num_sessions=num_sessions):
                logger.error("‚ùå Falha ao inicializar TDLib SessionPool")
                return

            # 3Ô∏è‚É£ Busca informa√ß√µes do chat
            try:
                chat_info = await session_pool.get_chat(group["id"])
                logger.info(f"‚úÖ Chat conectado: {chat_info.get('title', 'N/A')}")
            except Exception as e:
                logger.error(f"‚ùå Erro ao buscar chat: {e}")
                return

            # 4Ô∏è‚É£ Busca mensagens n√£o processadas
            messages_to_process = await self._fetch_unprocessed_messages(
                session_pool, group
            )

            if not messages_to_process:
                logger.info("‚úÖ Nenhuma mensagem nova para processar")
                return

            # Limita ao total configurado
            total_expected = self.num_workers * self.msg_per_worker
            if len(messages_to_process) > total_expected:
                logger.info(f"üìä Limitando processamento a {total_expected} mensagens")
                messages_to_process = messages_to_process[:total_expected]

            # 5Ô∏è‚É£ Processa mensagens com alta concorr√™ncia
            await self._process_messages_parallel(
                session_pool, group, messages_to_process
            )

        finally:
            await session_pool.close_all()

            elapsed = time.time() - start_time
            logger.info(f"\n‚è±Ô∏è  Grupo finalizado em {elapsed:.0f}s ({elapsed/60:.1f} min)")

    async def _fetch_unprocessed_messages(
            self,
            session_pool: TDLibSessionPool,
            group: dict
    ) -> List[dict]:
        """
        Busca mensagens n√£o processadas do grupo

        TDLib usa pagina√ß√£o diferente:
        - from_message_id ao inv√©s de offset_id
        - Retorna lista de dicts ao inv√©s de objetos Message
        """
        last_id = group.get("last_update_id", 0)
        total_to_fetch = self.num_workers * self.msg_per_worker

        logger.info(f"üîç Buscando mensagens ap√≥s ID {last_id}...")
        logger.info(f"üìä Meta: {total_to_fetch} mensagens n√£o processadas")

        # Busca IDs j√° processados (otimiza√ß√£o)
        processed_ids = set(
            self.media_repo.where("group_id", group["id"]).pluck("telegram_message_id")
        )
        logger.info(f"üìä {len(processed_ids)} mensagens j√° no banco")

        unprocessed_messages = []
        batch_size = TDLibConfig.BATCH_SIZE  # 300 por padr√£o
        from_message_id = last_id

        fetch_start = time.time()

        while len(unprocessed_messages) < total_to_fetch:
            try:
                # TDLib: busca batch de mensagens
                messages_batch = await session_pool.get_messages_batch(
                    chat_id=group["id"],
                    limit=batch_size,
                    from_message_id=from_message_id
                )

                if not messages_batch:
                    logger.info("üì≠ Fim do hist√≥rico de mensagens")
                    break

                # Filtra mensagens com m√≠dia n√£o processadas
                for msg in messages_batch:
                    msg_id = msg.get("id")
                    content = msg.get("content", {})

                    # Verifica se tem m√≠dia
                    has_media = self._message_has_media(content)

                    if has_media and msg_id not in processed_ids:
                        unprocessed_messages.append(msg)

                        if len(unprocessed_messages) >= total_to_fetch:
                            break

                # Atualiza posi√ß√£o para pr√≥ximo batch
                if messages_batch:
                    from_message_id = messages_batch[-1].get("id", 0)

                # Log de progresso
                if len(unprocessed_messages) % 100 == 0 and len(unprocessed_messages) > 0:
                    logger.info(f"  üì• {len(unprocessed_messages)} mensagens coletadas...")

                if len(unprocessed_messages) >= total_to_fetch:
                    break

                # TDLib: pausa m√≠nima (muito menor que Telethon)
                await asyncio.sleep(0.3)

            except Exception as e:
                logger.error(f"‚ùå Erro ao buscar batch: {e}")
                break

        fetch_elapsed = time.time() - fetch_start
        logger.info(f"‚úÖ {len(unprocessed_messages)} mensagens encontradas em {fetch_elapsed:.1f}s")

        return unprocessed_messages

    def _message_has_media(self, content: dict) -> bool:
        """Verifica se mensagem tem m√≠dia suportada"""
        content_type = content.get("@type", "")

        return content_type in [
            "messagePhoto",
            "messageVideo",
            "messageDocument",
            "messageAnimation"
        ]

    async def _process_messages_parallel(
            self,
            session_pool: TDLibSessionPool,
            group: dict,
            messages: List[dict]
    ):
        """
        Processamento paralelo de mensagens com TDLib

        Performance:
        - 50+ downloads simult√¢neos
        - ~4000-6000 msgs/hora
        - Rate limits muito menores
        """
        from app.services.pipeline_service import PipelineService

        total = len(messages)
        logger.info(f"\n{'='*70}")
        logger.info(f"üöÄ Iniciando processamento paralelo")
        logger.info(f"üìä Total: {total} mensagens")
        logger.info(f"‚ö° Concorr√™ncia: {self.max_concurrent_downloads} downloads simult√¢neos")
        logger.info(f"{'='*70}\n")

        # Status do pool
        pool_status = session_pool.get_pool_status()
        logger.info(f"üìä Pool TDLib: {pool_status['available_sessions']}/{pool_status['total_sessions']} sess√µes")

        pipeline = PipelineService()

        # Controle de concorr√™ncia (TDLib aguenta muito mais)
        semaphore = asyncio.Semaphore(self.max_concurrent_downloads)

        # Contadores
        processed = 0
        failed = 0

        async def _process_one(msg: dict, idx: int):
            """Processa uma mensagem"""
            nonlocal processed, failed

            async with semaphore:
                msg_id = msg.get("id")

                try:
                    # 1Ô∏è‚É£ Download com TDLib (muito mais r√°pido)
                    file_bytes = await session_pool.download_media(msg)

                    if not file_bytes:
                        logger.warning(f"‚ö†Ô∏è Download falhou para mensagem {msg_id}")
                        failed += 1
                        return

                    # 2Ô∏è‚É£ Extrai mime type
                    mime = self._extract_mime_type(msg)

                    # 3Ô∏è‚É£ Pipeline (seu c√≥digo existente)
                    await pipeline.process_message(
                        msg=self._convert_to_telethon_format(msg),  # Adapta formato
                        file_bytes=file_bytes,
                        mime=mime,
                        group=group,
                        worker_id=idx % self.num_workers
                    )

                    processed += 1

                    # Log de progresso (a cada 50 mensagens)
                    if processed % 50 == 0:
                        elapsed = time.time() - start_time
                        rate = processed / elapsed if elapsed > 0 else 0
                        progress = (processed * 100) // total if total > 0 else 0

                        logger.info(
                            f"  üìä {processed}/{total} ({progress}%) | "
                            f"{rate:.1f} msgs/s | "
                            f"{rate*60:.0f} msgs/min"
                        )

                except Exception as e:
                    logger.error(f"‚ùå Erro processando mensagem {msg_id}: {e}")
                    failed += 1

        # Executa processamento paralelo
        start_time = time.time()

        tasks = [_process_one(msg, idx) for idx, msg in enumerate(messages)]
        await asyncio.gather(*tasks, return_exceptions=True)

        elapsed = time.time() - start_time

        # üìä Relat√≥rio final
        logger.info(f"\n{'='*70}")
        logger.info(f"‚úÖ PROCESSAMENTO CONCLU√çDO")
        logger.info(f"{'='*70}")
        logger.info(f"üìä Processadas: {processed}/{total} ({processed*100//total if total > 0 else 0}%)")
        logger.info(f"‚ùå Falhas: {failed}")
        logger.info(f"‚è±Ô∏è  Tempo: {elapsed:.0f}s ({elapsed/60:.1f} min)")

        if elapsed > 0:
            rate = processed / elapsed
            logger.info(f"‚ö° Velocidade:")
            logger.info(f"   ‚Ä¢ {rate:.2f} msgs/segundo")
            logger.info(f"   ‚Ä¢ {rate*60:.0f} msgs/minuto")
            logger.info(f"   ‚Ä¢ {rate*3600:.0f} msgs/hora estimadas")

        # Status final do pool
        final_status = session_pool.get_pool_status()
        logger.info(f"üîÑ Total de downloads: {final_status['total_downloads']}")
        logger.info(f"üîÑ Total de requisi√ß√µes: {final_status['total_requests']}")
        logger.info(f"{'='*70}\n")

        # üíæ Atualiza last_update_id no banco
        if messages and processed > 0:
            max_id = max(msg.get("id", 0) for msg in messages)

            if max_id > group.get("last_update_id", 0):
                self.groups_repo.update(group["id"], {"last_update_id": max_id})
                logger.info(f"üíæ Last_update_id atualizado: {max_id}")

    def _extract_mime_type(self, message: dict) -> str:
        """Extrai mime type de uma mensagem TDLib"""
        content = message.get("content", {})
        content_type = content.get("@type", "")

        if content_type == "messagePhoto":
            return "image/jpeg"

        elif content_type == "messageVideo":
            mime = content.get("video", {}).get("mime_type", "video/mp4")
            return mime

        elif content_type == "messageDocument":
            mime = content.get("document", {}).get("mime_type", "application/octet-stream")
            return mime

        elif content_type == "messageAnimation":
            return "image/gif"

        return "application/octet-stream"

    def _convert_to_telethon_format(self, tdlib_msg: dict):
        """
        Converte mensagem TDLib para formato compat√≠vel com pipeline

        Seu pipeline espera atributos como msg.id, msg.date, msg.file
        Criamos um objeto dict que simula esses atributos
        """
        from datetime import datetime

        # Cria objeto compat√≠vel
        class TDLibMessageAdapter:
            def __init__(self, tdlib_data):
                self.id = tdlib_data.get("id")
                self.date = datetime.fromtimestamp(tdlib_data.get("date", 0))
                self.chat_id = tdlib_data.get("chat_id")

                # Simula atributo file (usado no pipeline)
                content = tdlib_data.get("content", {})
                self.file = type('obj', (object,), {
                    'mime_type': self._get_mime(content)
                })()

            def _get_mime(self, content):
                content_type = content.get("@type", "")

                if content_type == "messagePhoto":
                    return "image/jpeg"
                elif content_type == "messageVideo":
                    return content.get("video", {}).get("mime_type", "video/mp4")
                elif content_type == "messageDocument":
                    return content.get("document", {}).get("mime_type", "application/octet-stream")
                elif content_type == "messageAnimation":
                    return "image/gif"

                return "application/octet-stream"

        return TDLibMessageAdapter(tdlib_msg)